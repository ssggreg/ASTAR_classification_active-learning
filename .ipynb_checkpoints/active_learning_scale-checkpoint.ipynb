{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from vogn import VOGN\n",
    "from models import SimpleConvNet\n",
    "from datasets import Dataset\n",
    "from utils import train_model\n",
    "from utils import inference\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "sns.set()\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : attention aux colums de file_A et file, columns en plus etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Cuda: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f15aec79bd0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Test wether GPUs are available\n",
    "use_cuda =  torch.cuda.is_available()\n",
    "print(\"Using Cuda: %s\" % use_cuda)\n",
    "\n",
    "# Set Random Seed\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class csvDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data,label, transform=None):\n",
    "        \n",
    "        self.label = label\n",
    "                \n",
    "        self.data = data\n",
    "         \n",
    "        #self.train_set = TensorDataset()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = self.data[idx]\n",
    "        label = self.label[idx]\n",
    "        sample = { 'data': data,'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        data, label  = sample['data'],sample['label']\n",
    "        \n",
    "        \n",
    "        return {'data': torch.from_numpy(data).float(),\n",
    "                'label': torch.from_numpy(label).float()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(type(self), self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(16, 64)\n",
    "        self.layer2 = nn.Linear(64, 256)\n",
    "        self.layer3 = nn.Linear(256, 64)\n",
    "        self.layer4 = nn.Linear(64, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        out = self.layer4(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.random import seed\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_A=pd.read_csv('circuit-design/opAmp_280nm_GF55.csv')\n",
    "file = pd.read_csv('circuit-design/opAmp_280nm_GF55_Mod_30P.csv')\n",
    "        \n",
    "b=['Pass/Fail']\n",
    "\n",
    "label = np.array(file[b].values=='Fail')*(-1.)+1\n",
    "\n",
    "b = []\n",
    "\n",
    "for i in list(file_A.columns):\n",
    "    if i != 'Pass/Fail':\n",
    "        b.append(i)\n",
    "\n",
    "data = np.array(file[b].values)\n",
    "for i in range(16):\n",
    "    data[:,i]=(data[:,i] - data[:,i].mean())/(data[:,i].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_tmp, testX, trainY_tmp, testY  = train_test_split(data, label, test_size=0.2, random_state=1)\n",
    "trainX_tmp, valX, trainY_tmp, valY = train_test_split(trainX_tmp, trainY_tmp, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6912, 16), (6912, 1))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate((trainX_tmp,valX,testX),axis=0)\n",
    "Y = np.concatenate((trainY_tmp,valY,testY),axis=0)\n",
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dataset = csvDataset(X,Y,transform= ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader = torch.utils.data.DataLoader(file_dataset,\n",
    "                                             batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_loader = torch.utils.data.DataLoader(file_dataset,\n",
    "                                             batch_size=6912, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_bb(liste,model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    \"\"\"\n",
    "    Performs Training and Validation on test set on the given model using the specified optimizer\n",
    "    :param model: (nn.Module) Model to be trained\n",
    "    :param dataloaders: (list) train and test dataloaders\n",
    "    :param criterion: Loss Function\n",
    "    :param optimizer: Optimizer to be used for training\n",
    "    :param num_epochs: Number of epochs to train the model\n",
    "    :return: trained model, test and train metric history\n",
    "    \"\"\"\n",
    "    trainloader, testloader = dataloaders\n",
    "    for epoch in range(num_epochs):\n",
    "        listee = liste.copy()\n",
    "        d = listee.pop()\n",
    "        model.train(True)\n",
    "        print('Epoch[%d]:' % epoch)\n",
    "        running_train_loss = 0.\n",
    "        a=-1\n",
    "        for i in (trainloader):\n",
    "            a+=1\n",
    "            if (a==d):\n",
    "                inputs = i['data']\n",
    "                labels = i['label']\n",
    "                if use_cuda:\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                if isinstance(optimizer, VOGN):\n",
    "                    def closure():\n",
    "                        optimizer.zero_grad()\n",
    "                        logits = model.forward(inputs)\n",
    "                        loss = criterion(logits, labels)\n",
    "                        return loss\n",
    "                else:\n",
    "                    def closure():\n",
    "                        optimizer.zero_grad()\n",
    "                        logits = model.forward(inputs)\n",
    "                        loss = criterion(logits, labels)\n",
    "                        loss.backward()\n",
    "                        return loss\n",
    "                loss = optimizer.step(closure)\n",
    "                running_train_loss += loss.detach().item()\n",
    "                if len(listee)!=0:\n",
    "                    d = listee.pop()\n",
    "                else :\n",
    "                    break\n",
    "\n",
    "            # Print Training Progress\n",
    "            if a%500 == 100:\n",
    "                train_accuracy = accuracy_bb(model, trainloader)\n",
    "                print('Iteration[%d]:  Train Accuracy: %f ' % (a+1, train_accuracy))\n",
    "\n",
    "        train_accuracy, train_loss = accuracy_bb(model, trainloader, criterion)\n",
    "        print('## Epoch[%d], Train Loss: %f   &   Train Accuracy: %f' % (epoch, train_loss, train_accuracy))\n",
    "    return model, train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_cc(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    \"\"\"\n",
    "    Performs Training and Validation on test set on the given model using the specified optimizer\n",
    "    :param model: (nn.Module) Model to be trained\n",
    "    :param dataloaders: (list) train and test dataloaders\n",
    "    :param criterion: Loss Function\n",
    "    :param optimizer: Optimizer to be used for training\n",
    "    :param num_epochs: Number of epochs to train the model\n",
    "    :return: trained model, test and train metric history\n",
    "    \"\"\"\n",
    "    trainloader, testloader = dataloaders\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train(True)\n",
    "        print('Epoch[%d]:' % epoch)\n",
    "        running_train_loss = 0.\n",
    "        a=-1\n",
    "        for i in (trainloader):\n",
    "            a+=1\n",
    "            inputs = i['data']\n",
    "            labels = i['label']\n",
    "            if use_cuda:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            if isinstance(optimizer, VOGN):\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    logits = model.forward(inputs)\n",
    "                    loss = criterion(logits, labels)\n",
    "                    return loss\n",
    "            else:\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    logits = model.forward(inputs)\n",
    "                    loss = criterion(logits, labels)\n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "            loss = optimizer.step(closure)\n",
    "            running_train_loss += loss.detach().item()\n",
    "\n",
    "\n",
    "            # Print Training Progress\n",
    "            if a%500 == 100:\n",
    "                train_accuracy = accuracy_bb(model, trainloader)\n",
    "                print('Iteration[%d]:  Train Accuracy: %f ' % (a+1, train_accuracy))\n",
    "\n",
    "        train_accuracy, train_loss = accuracy_bb(model, trainloader, criterion)\n",
    "        print('## Epoch[%d], Train Loss: %f   &   Train Accuracy: %f' % (epoch, train_loss, train_accuracy))\n",
    "    return model, train_loss, train_accuracy\n",
    "\n",
    "def f (x):\n",
    "    return x*(1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_bb(model, dataloader, criterion=None):\n",
    "    \"\"\" Computes the model's classification accuracy on the train dataset\n",
    "        Computes classification accuracy and loss(optional) on the test dataset\n",
    "        The model should return logits\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0.\n",
    "        running_loss = 0.\n",
    "        for i in (dataloader):\n",
    "            inputs = i['data']\n",
    "            labels = i['label']\n",
    "            if use_cuda:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            if criterion is not None:\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "            pred = (outputs>0).float()\n",
    "            correct += (pred.view(-1,1) == labels).sum().item()\n",
    "        accuracy = correct / len(dataloader.dataset)\n",
    "        if criterion is not None:\n",
    "            running_loss = running_loss / len(dataloader)\n",
    "            return accuracy, loss\n",
    "    return accuracy\n",
    "\n",
    "def inference_bb(model, data_loader, optimizer,mc_samples):\n",
    "    a=0\n",
    "    for i in (data_loader):\n",
    "            inputs = i['data']\n",
    "            labels = i['label']\n",
    "            \n",
    "    if use_cuda:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                \n",
    "    return optimizer.get_mc_predictions(model.forward,inputs,mc_samples=mc_samples)[0]>0,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0]:\n",
      "## Epoch[0], Train Loss: 0.640918   &   Train Accuracy: 0.767361\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.750868 \n",
      "Iteration[601]:  Train Accuracy: 0.413050 \n",
      "Iteration[1101]:  Train Accuracy: 0.413050 \n",
      "Iteration[1601]:  Train Accuracy: 0.413050 \n",
      "Iteration[2101]:  Train Accuracy: 0.326100 \n",
      "Iteration[2601]:  Train Accuracy: 0.309317 \n",
      "Iteration[3101]:  Train Accuracy: 0.309317 \n",
      "Iteration[3601]:  Train Accuracy: 0.309317 \n",
      "## Epoch[0], Train Loss: 0.926853   &   Train Accuracy: 0.309317\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.669560 \n",
      "Iteration[601]:  Train Accuracy: 0.652054 \n",
      "Iteration[1101]:  Train Accuracy: 0.309317 \n",
      "Iteration[1601]:  Train Accuracy: 0.690683 \n",
      "Iteration[2101]:  Train Accuracy: 0.690683 \n",
      "Iteration[2601]:  Train Accuracy: 0.339699 \n",
      "Iteration[3101]:  Train Accuracy: 0.698206 \n",
      "Iteration[3601]:  Train Accuracy: 0.690683 \n",
      "## Epoch[0], Train Loss: 0.601618   &   Train Accuracy: 0.691696\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.568866 \n",
      "Iteration[601]:  Train Accuracy: 0.690683 \n",
      "Iteration[1101]:  Train Accuracy: 0.696759 \n",
      "Iteration[1601]:  Train Accuracy: 0.690683 \n",
      "Iteration[2101]:  Train Accuracy: 0.690683 \n",
      "Iteration[2601]:  Train Accuracy: 0.337963 \n",
      "Iteration[3101]:  Train Accuracy: 0.758681 \n",
      "Iteration[3601]:  Train Accuracy: 0.690683 \n",
      "## Epoch[0], Train Loss: 0.579650   &   Train Accuracy: 0.690683\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.347512 \n",
      "Iteration[601]:  Train Accuracy: 0.723235 \n",
      "Iteration[1101]:  Train Accuracy: 0.363281 \n",
      "Iteration[1601]:  Train Accuracy: 0.736834 \n",
      "Iteration[2101]:  Train Accuracy: 0.706163 \n",
      "Iteration[2601]:  Train Accuracy: 0.309317 \n",
      "Iteration[3101]:  Train Accuracy: 0.503906 \n",
      "Iteration[3601]:  Train Accuracy: 0.535156 \n",
      "## Epoch[0], Train Loss: 0.633235   &   Train Accuracy: 0.634259\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.641927 \n",
      "Iteration[601]:  Train Accuracy: 0.706019 \n",
      "Iteration[1101]:  Train Accuracy: 0.309317 \n",
      "Iteration[1601]:  Train Accuracy: 0.310041 \n",
      "Iteration[2101]:  Train Accuracy: 0.410301 \n",
      "Iteration[2601]:  Train Accuracy: 0.309317 \n",
      "Iteration[3101]:  Train Accuracy: 0.309317 \n",
      "Iteration[3601]:  Train Accuracy: 0.507234 \n",
      "Iteration[4101]:  Train Accuracy: 0.561198 \n",
      "## Epoch[0], Train Loss: 0.658571   &   Train Accuracy: 0.716869\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.473090 \n",
      "Iteration[601]:  Train Accuracy: 0.574074 \n",
      "Iteration[1101]:  Train Accuracy: 0.309317 \n",
      "Iteration[1601]:  Train Accuracy: 0.309317 \n",
      "Iteration[2101]:  Train Accuracy: 0.309317 \n",
      "Iteration[2601]:  Train Accuracy: 0.309317 \n",
      "Iteration[3101]:  Train Accuracy: 0.405093 \n",
      "Iteration[3601]:  Train Accuracy: 0.533709 \n",
      "Iteration[4101]:  Train Accuracy: 0.715856 \n",
      "## Epoch[0], Train Loss: 0.588245   &   Train Accuracy: 0.795139\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.686343 \n",
      "Iteration[601]:  Train Accuracy: 0.538194 \n",
      "Iteration[1101]:  Train Accuracy: 0.309317 \n",
      "Iteration[1601]:  Train Accuracy: 0.309317 \n",
      "Iteration[2101]:  Train Accuracy: 0.309317 \n",
      "Iteration[2601]:  Train Accuracy: 0.309317 \n",
      "Iteration[3101]:  Train Accuracy: 0.310041 \n",
      "Iteration[3601]:  Train Accuracy: 0.364439 \n",
      "Iteration[4101]:  Train Accuracy: 0.380208 \n",
      "## Epoch[0], Train Loss: 0.728444   &   Train Accuracy: 0.535301\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.310185 \n",
      "Iteration[1101]:  Train Accuracy: 0.309317 \n",
      "Iteration[1601]:  Train Accuracy: 0.309317 \n",
      "Iteration[2101]:  Train Accuracy: 0.351128 \n",
      "Iteration[2601]:  Train Accuracy: 0.309317 \n",
      "Iteration[3101]:  Train Accuracy: 0.309317 \n",
      "Iteration[3601]:  Train Accuracy: 0.375579 \n",
      "Iteration[4101]:  Train Accuracy: 0.364583 \n",
      "## Epoch[0], Train Loss: 0.806321   &   Train Accuracy: 0.473669\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.701823 \n",
      "Iteration[601]:  Train Accuracy: 0.352575 \n",
      "Iteration[1101]:  Train Accuracy: 0.309317 \n",
      "Iteration[1601]:  Train Accuracy: 0.309317 \n",
      "Iteration[2101]:  Train Accuracy: 0.318142 \n",
      "Iteration[2601]:  Train Accuracy: 0.379485 \n",
      "Iteration[3101]:  Train Accuracy: 0.637008 \n",
      "Iteration[3601]:  Train Accuracy: 0.755642 \n",
      "Iteration[4101]:  Train Accuracy: 0.743634 \n",
      "## Epoch[0], Train Loss: 0.333373   &   Train Accuracy: 0.773727\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.687789 \n",
      "Iteration[601]:  Train Accuracy: 0.688368 \n",
      "Iteration[1101]:  Train Accuracy: 0.309317 \n",
      "Iteration[1601]:  Train Accuracy: 0.309317 \n",
      "Iteration[2101]:  Train Accuracy: 0.460793 \n",
      "Iteration[2601]:  Train Accuracy: 0.316117 \n",
      "Iteration[3101]:  Train Accuracy: 0.648582 \n",
      "Iteration[3601]:  Train Accuracy: 0.763021 \n",
      "Iteration[4101]:  Train Accuracy: 0.681858 \n",
      "## Epoch[0], Train Loss: 0.461619   &   Train Accuracy: 0.734375\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.695457 \n",
      "Iteration[1101]:  Train Accuracy: 0.532407 \n",
      "Iteration[1601]:  Train Accuracy: 0.312355 \n",
      "Iteration[2101]:  Train Accuracy: 0.323640 \n",
      "Iteration[2601]:  Train Accuracy: 0.309317 \n",
      "Iteration[3101]:  Train Accuracy: 0.523003 \n",
      "Iteration[3601]:  Train Accuracy: 0.665365 \n",
      "Iteration[4101]:  Train Accuracy: 0.693142 \n",
      "## Epoch[0], Train Loss: 0.658094   &   Train Accuracy: 0.751591\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.692130 \n",
      "Iteration[601]:  Train Accuracy: 0.396412 \n",
      "Iteration[1101]:  Train Accuracy: 0.393374 \n",
      "Iteration[1601]:  Train Accuracy: 0.309317 \n",
      "Iteration[2101]:  Train Accuracy: 0.313947 \n",
      "Iteration[2601]:  Train Accuracy: 0.309317 \n",
      "Iteration[3101]:  Train Accuracy: 0.511140 \n",
      "Iteration[3601]:  Train Accuracy: 0.698351 \n",
      "Iteration[4101]:  Train Accuracy: 0.734375 \n",
      "## Epoch[0], Train Loss: 0.491825   &   Train Accuracy: 0.766059\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.744358 \n",
      "Iteration[1101]:  Train Accuracy: 0.595197 \n",
      "Iteration[1601]:  Train Accuracy: 0.512442 \n",
      "Iteration[2101]:  Train Accuracy: 0.343027 \n",
      "Iteration[2601]:  Train Accuracy: 0.309317 \n",
      "Iteration[3101]:  Train Accuracy: 0.633536 \n",
      "Iteration[3601]:  Train Accuracy: 0.750723 \n",
      "Iteration[4101]:  Train Accuracy: 0.509838 \n",
      "## Epoch[0], Train Loss: 0.680563   &   Train Accuracy: 0.627749\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.693721 \n",
      "Iteration[1101]:  Train Accuracy: 0.746383 \n",
      "Iteration[1601]:  Train Accuracy: 0.740596 \n",
      "Iteration[2101]:  Train Accuracy: 0.453125 \n",
      "Iteration[2601]:  Train Accuracy: 0.309317 \n",
      "Iteration[3101]:  Train Accuracy: 0.492766 \n",
      "Iteration[3601]:  Train Accuracy: 0.709346 \n",
      "Iteration[4101]:  Train Accuracy: 0.532552 \n",
      "## Epoch[0], Train Loss: 0.556992   &   Train Accuracy: 0.600550\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.693287 \n",
      "Iteration[1101]:  Train Accuracy: 0.382234 \n",
      "Iteration[1601]:  Train Accuracy: 0.424334 \n",
      "Iteration[2101]:  Train Accuracy: 0.314381 \n",
      "Iteration[2601]:  Train Accuracy: 0.309317 \n",
      "Iteration[3101]:  Train Accuracy: 0.678096 \n",
      "Iteration[3601]:  Train Accuracy: 0.731771 \n",
      "Iteration[4101]:  Train Accuracy: 0.616898 \n",
      "## Epoch[0], Train Loss: 0.601027   &   Train Accuracy: 0.667824\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.582610 \n",
      "Iteration[1101]:  Train Accuracy: 0.314236 \n",
      "Iteration[1601]:  Train Accuracy: 0.733073 \n",
      "Iteration[2101]:  Train Accuracy: 0.432147 \n",
      "Iteration[2601]:  Train Accuracy: 0.506655 \n",
      "Iteration[3101]:  Train Accuracy: 0.772859 \n",
      "Iteration[3601]:  Train Accuracy: 0.802517 \n",
      "Iteration[4101]:  Train Accuracy: 0.643663 \n",
      "## Epoch[0], Train Loss: 0.649327   &   Train Accuracy: 0.712384\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690972 \n",
      "Iteration[601]:  Train Accuracy: 0.704861 \n",
      "Iteration[1101]:  Train Accuracy: 0.309317 \n",
      "Iteration[1601]:  Train Accuracy: 0.718895 \n",
      "Iteration[2101]:  Train Accuracy: 0.315249 \n",
      "Iteration[2601]:  Train Accuracy: 0.309317 \n",
      "Iteration[3101]:  Train Accuracy: 0.476562 \n",
      "Iteration[3601]:  Train Accuracy: 0.720052 \n",
      "Iteration[4101]:  Train Accuracy: 0.382812 \n",
      "## Epoch[0], Train Loss: 0.485572   &   Train Accuracy: 0.437211\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.743490 \n",
      "Iteration[1101]:  Train Accuracy: 0.706887 \n",
      "Iteration[1601]:  Train Accuracy: 0.692419 \n",
      "Iteration[2101]:  Train Accuracy: 0.482784 \n",
      "Iteration[2601]:  Train Accuracy: 0.313368 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration[3101]:  Train Accuracy: 0.714554 \n",
      "Iteration[3601]:  Train Accuracy: 0.733362 \n",
      "Iteration[4101]:  Train Accuracy: 0.635561 \n",
      "## Epoch[0], Train Loss: 0.436291   &   Train Accuracy: 0.690972\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.761140 \n",
      "Iteration[1101]:  Train Accuracy: 0.687645 \n",
      "Iteration[1601]:  Train Accuracy: 0.767795 \n",
      "Iteration[2101]:  Train Accuracy: 0.620515 \n",
      "Iteration[2601]:  Train Accuracy: 0.573929 \n",
      "Iteration[3101]:  Train Accuracy: 0.778212 \n",
      "Iteration[3601]:  Train Accuracy: 0.793692 \n",
      "Iteration[4101]:  Train Accuracy: 0.670718 \n",
      "## Epoch[0], Train Loss: 0.526702   &   Train Accuracy: 0.718605\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.748698 \n",
      "Iteration[1101]:  Train Accuracy: 0.672598 \n",
      "Iteration[1601]:  Train Accuracy: 0.734954 \n",
      "Iteration[2101]:  Train Accuracy: 0.751881 \n",
      "Iteration[2601]:  Train Accuracy: 0.625000 \n",
      "Iteration[3101]:  Train Accuracy: 0.784722 \n",
      "Iteration[3601]:  Train Accuracy: 0.792969 \n",
      "Iteration[4101]:  Train Accuracy: 0.735243 \n",
      "## Epoch[0], Train Loss: 0.341461   &   Train Accuracy: 0.754340\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.690538 \n",
      "Iteration[1101]:  Train Accuracy: 0.678675 \n",
      "Iteration[1601]:  Train Accuracy: 0.690683 \n",
      "Iteration[2101]:  Train Accuracy: 0.709346 \n",
      "Iteration[2601]:  Train Accuracy: 0.309317 \n",
      "Iteration[3101]:  Train Accuracy: 0.763310 \n",
      "Iteration[3601]:  Train Accuracy: 0.758391 \n",
      "Iteration[4101]:  Train Accuracy: 0.526331 \n",
      "## Epoch[0], Train Loss: 0.675908   &   Train Accuracy: 0.606047\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.784144 \n",
      "Iteration[1101]:  Train Accuracy: 0.745081 \n",
      "Iteration[1601]:  Train Accuracy: 0.782697 \n",
      "Iteration[2101]:  Train Accuracy: 0.707899 \n",
      "Iteration[2601]:  Train Accuracy: 0.545718 \n",
      "Iteration[3101]:  Train Accuracy: 0.806568 \n",
      "Iteration[3601]:  Train Accuracy: 0.785012 \n",
      "Iteration[4101]:  Train Accuracy: 0.682726 \n",
      "## Epoch[0], Train Loss: 0.219328   &   Train Accuracy: 0.735388\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.702691 \n",
      "Iteration[1101]:  Train Accuracy: 0.688802 \n",
      "Iteration[1601]:  Train Accuracy: 0.690683 \n",
      "Iteration[2101]:  Train Accuracy: 0.582899 \n",
      "Iteration[2601]:  Train Accuracy: 0.309317 \n",
      "Iteration[3101]:  Train Accuracy: 0.776765 \n",
      "Iteration[3601]:  Train Accuracy: 0.416811 \n",
      "Iteration[4101]:  Train Accuracy: 0.350260 \n",
      "## Epoch[0], Train Loss: 0.598021   &   Train Accuracy: 0.437500\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.769097 \n",
      "Iteration[1101]:  Train Accuracy: 0.722946 \n",
      "Iteration[1601]:  Train Accuracy: 0.690683 \n",
      "Iteration[2101]:  Train Accuracy: 0.769242 \n",
      "Iteration[2601]:  Train Accuracy: 0.550781 \n",
      "Iteration[3101]:  Train Accuracy: 0.801071 \n",
      "Iteration[3601]:  Train Accuracy: 0.661314 \n",
      "Iteration[4101]:  Train Accuracy: 0.702402 \n",
      "## Epoch[0], Train Loss: 0.264154   &   Train Accuracy: 0.747541\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.575521 \n",
      "Iteration[1101]:  Train Accuracy: 0.422888 \n",
      "Iteration[1601]:  Train Accuracy: 0.686487 \n",
      "Iteration[2101]:  Train Accuracy: 0.567419 \n",
      "Iteration[2601]:  Train Accuracy: 0.489149 \n",
      "Iteration[3101]:  Train Accuracy: 0.786892 \n",
      "Iteration[3601]:  Train Accuracy: 0.538194 \n",
      "Iteration[4101]:  Train Accuracy: 0.493924 \n",
      "## Epoch[0], Train Loss: 0.613992   &   Train Accuracy: 0.552951\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.705584 \n",
      "Iteration[1101]:  Train Accuracy: 0.694444 \n",
      "Iteration[1601]:  Train Accuracy: 0.713686 \n",
      "Iteration[2101]:  Train Accuracy: 0.701968 \n",
      "Iteration[2601]:  Train Accuracy: 0.724537 \n",
      "Iteration[3101]:  Train Accuracy: 0.741030 \n",
      "Iteration[3601]:  Train Accuracy: 0.752604 \n",
      "Iteration[4101]:  Train Accuracy: 0.693721 \n",
      "## Epoch[0], Train Loss: 0.233661   &   Train Accuracy: 0.759404\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.702402 \n",
      "Iteration[1101]:  Train Accuracy: 0.611256 \n",
      "Iteration[1601]:  Train Accuracy: 0.760561 \n",
      "Iteration[2101]:  Train Accuracy: 0.310619 \n",
      "Iteration[2601]:  Train Accuracy: 0.309317 \n",
      "Iteration[3101]:  Train Accuracy: 0.720486 \n",
      "Iteration[3601]:  Train Accuracy: 0.554109 \n",
      "Iteration[4101]:  Train Accuracy: 0.573495 \n",
      "## Epoch[0], Train Loss: 0.400007   &   Train Accuracy: 0.685475\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.721499 \n",
      "Iteration[1101]:  Train Accuracy: 0.571181 \n",
      "Iteration[1601]:  Train Accuracy: 0.782697 \n",
      "Iteration[2101]:  Train Accuracy: 0.580150 \n",
      "Iteration[2601]:  Train Accuracy: 0.654080 \n",
      "Iteration[3101]:  Train Accuracy: 0.822772 \n",
      "Iteration[3601]:  Train Accuracy: 0.711806 \n",
      "Iteration[4101]:  Train Accuracy: 0.670139 \n",
      "## Epoch[0], Train Loss: 0.348652   &   Train Accuracy: 0.715278\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.582899 \n",
      "Iteration[1101]:  Train Accuracy: 0.710503 \n",
      "Iteration[1601]:  Train Accuracy: 0.508970 \n",
      "Iteration[2101]:  Train Accuracy: 0.453414 \n",
      "Iteration[2601]:  Train Accuracy: 0.385417 \n",
      "Iteration[3101]:  Train Accuracy: 0.767216 \n",
      "Iteration[3601]:  Train Accuracy: 0.641927 \n",
      "Iteration[4101]:  Train Accuracy: 0.545718 \n",
      "## Epoch[0], Train Loss: 0.260133   &   Train Accuracy: 0.598669\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.690683 \n",
      "Iteration[1101]:  Train Accuracy: 0.599103 \n",
      "Iteration[1601]:  Train Accuracy: 0.696036 \n",
      "Iteration[2101]:  Train Accuracy: 0.322193 \n",
      "Iteration[2601]:  Train Accuracy: 0.336661 \n",
      "Iteration[3101]:  Train Accuracy: 0.773003 \n",
      "Iteration[3601]:  Train Accuracy: 0.386140 \n",
      "Iteration[4101]:  Train Accuracy: 0.612413 \n",
      "## Epoch[0], Train Loss: 0.437695   &   Train Accuracy: 0.706887\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.710069 \n",
      "Iteration[1101]:  Train Accuracy: 0.700231 \n",
      "Iteration[1601]:  Train Accuracy: 0.690683 \n",
      "Iteration[2101]:  Train Accuracy: 0.679253 \n",
      "Iteration[2601]:  Train Accuracy: 0.801071 \n",
      "Iteration[3101]:  Train Accuracy: 0.702691 \n",
      "Iteration[3601]:  Train Accuracy: 0.765046 \n",
      "Iteration[4101]:  Train Accuracy: 0.774884 \n",
      "## Epoch[0], Train Loss: 0.426545   &   Train Accuracy: 0.768953\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.751157 \n",
      "Iteration[1101]:  Train Accuracy: 0.422598 \n",
      "Iteration[1601]:  Train Accuracy: 0.725694 \n",
      "Iteration[2101]:  Train Accuracy: 0.443432 \n",
      "Iteration[2601]:  Train Accuracy: 0.618634 \n",
      "Iteration[3101]:  Train Accuracy: 0.764178 \n",
      "Iteration[3601]:  Train Accuracy: 0.673611 \n",
      "Iteration[4101]:  Train Accuracy: 0.570747 \n",
      "## Epoch[0], Train Loss: 0.621603   &   Train Accuracy: 0.601852\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.667535 \n",
      "Iteration[1101]:  Train Accuracy: 0.723235 \n",
      "Iteration[1601]:  Train Accuracy: 0.690828 \n",
      "Iteration[2101]:  Train Accuracy: 0.317853 \n",
      "Iteration[2601]:  Train Accuracy: 0.377749 \n",
      "Iteration[3101]:  Train Accuracy: 0.760995 \n",
      "Iteration[3601]:  Train Accuracy: 0.663339 \n",
      "Iteration[4101]:  Train Accuracy: 0.653067 \n",
      "## Epoch[0], Train Loss: 0.541497   &   Train Accuracy: 0.694300\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.535301 \n",
      "Iteration[1101]:  Train Accuracy: 0.655961 \n",
      "Iteration[1601]:  Train Accuracy: 0.716146 \n",
      "Iteration[2101]:  Train Accuracy: 0.465856 \n",
      "Iteration[2601]:  Train Accuracy: 0.684028 \n",
      "Iteration[3101]:  Train Accuracy: 0.802083 \n",
      "Iteration[3601]:  Train Accuracy: 0.739728 \n",
      "Iteration[4101]:  Train Accuracy: 0.704427 \n",
      "## Epoch[0], Train Loss: 0.500544   &   Train Accuracy: 0.708478\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.713252 \n",
      "Iteration[1101]:  Train Accuracy: 0.729167 \n",
      "Iteration[1601]:  Train Accuracy: 0.715712 \n",
      "Iteration[2101]:  Train Accuracy: 0.531829 \n",
      "Iteration[2601]:  Train Accuracy: 0.609230 \n",
      "Iteration[3101]:  Train Accuracy: 0.781829 \n",
      "Iteration[3601]:  Train Accuracy: 0.747106 \n",
      "Iteration[4101]:  Train Accuracy: 0.773148 \n",
      "## Epoch[0], Train Loss: 0.116359   &   Train Accuracy: 0.799624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.693287 \n",
      "Iteration[601]:  Train Accuracy: 0.506655 \n",
      "Iteration[1101]:  Train Accuracy: 0.544416 \n",
      "Iteration[1601]:  Train Accuracy: 0.695457 \n",
      "Iteration[2101]:  Train Accuracy: 0.648293 \n",
      "Iteration[2601]:  Train Accuracy: 0.746528 \n",
      "Iteration[3101]:  Train Accuracy: 0.799479 \n",
      "Iteration[3601]:  Train Accuracy: 0.707321 \n",
      "Iteration[4101]:  Train Accuracy: 0.772859 \n",
      "## Epoch[0], Train Loss: 0.290493   &   Train Accuracy: 0.777199\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.476997 \n",
      "Iteration[1101]:  Train Accuracy: 0.616753 \n",
      "Iteration[1601]:  Train Accuracy: 0.699797 \n",
      "Iteration[2101]:  Train Accuracy: 0.505208 \n",
      "Iteration[2601]:  Train Accuracy: 0.661603 \n",
      "Iteration[3101]:  Train Accuracy: 0.772135 \n",
      "Iteration[3601]:  Train Accuracy: 0.559606 \n",
      "Iteration[4101]:  Train Accuracy: 0.701678 \n",
      "## Epoch[0], Train Loss: 0.210639   &   Train Accuracy: 0.746238\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.309317 \n",
      "Iteration[1101]:  Train Accuracy: 0.401042 \n",
      "Iteration[1601]:  Train Accuracy: 0.629630 \n",
      "Iteration[2101]:  Train Accuracy: 0.495226 \n",
      "Iteration[2601]:  Train Accuracy: 0.735388 \n",
      "Iteration[3101]:  Train Accuracy: 0.813657 \n",
      "Iteration[3601]:  Train Accuracy: 0.750434 \n",
      "Iteration[4101]:  Train Accuracy: 0.716435 \n",
      "## Epoch[0], Train Loss: 0.225745   &   Train Accuracy: 0.740596\n",
      "99\n",
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.690683 \n",
      "Iteration[601]:  Train Accuracy: 0.438368 \n",
      "Iteration[1101]:  Train Accuracy: 0.687645 \n",
      "Iteration[1601]:  Train Accuracy: 0.707755 \n",
      "Iteration[2101]:  Train Accuracy: 0.659578 \n",
      "Iteration[2601]:  Train Accuracy: 0.759693 \n",
      "Iteration[3101]:  Train Accuracy: 0.750000 \n",
      "Iteration[3601]:  Train Accuracy: 0.771412 \n",
      "Iteration[4101]:  Train Accuracy: 0.777633 \n",
      "## Epoch[0], Train Loss: 0.007299   &   Train Accuracy: 0.794271\n",
      "99\n",
      "1382.119978427887\n"
     ]
    }
   ],
   "source": [
    "a = [11,10,9,8,7,6,5,4,3,2,1,0]\n",
    "labz=torch.zeros(100,6912).cuda()\n",
    "predict = torch.zeros(100,6912).cuda()\n",
    "start = time.time()\n",
    "for i in range(40):\n",
    "    model = SimpleConvNet()\n",
    "    if use_cuda:\n",
    "        model = model.float().cuda()\n",
    "    criterion = F.binary_cross_entropy_with_logits\n",
    "    optimizer = VOGN(model, train_set_size=6912, prec_init=1e2, num_samples=4)\n",
    "    model, train_loss, train_accuracy = train_model_bb(a,model, [dataset_loader, dataset_loader], criterion,\n",
    "optimizer, num_epochs=1)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(100):\n",
    "            predictions,lbl = inference_bb(model, inference_loader,optimizer,1)\n",
    "            predict[i] = predictions.view(6912)\n",
    "            labz[i] = lbl.view(6912)\n",
    "        \n",
    "    predict_train = np.sum(predict[:,:4146].cpu().numpy(),axis=0)/100    \n",
    "    BB =list(np.argsort(f(predict_train))[4136:])+a\n",
    "    BB.sort(reverse = True)\n",
    "\n",
    "    a = list(dict.fromkeys(BB))\n",
    "    print(i)    \n",
    "end = time.time()\n",
    "print(end - start)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VDD',\n",
       " 'Ib',\n",
       " 'Lg1',\n",
       " 'Lg2',\n",
       " 'Lrf',\n",
       " 'Wcf',\n",
       " 'Wg1',\n",
       " 'Wg2',\n",
       " 'temperature',\n",
       " 'ACM_G',\n",
       " 'SR',\n",
       " 'CMRR',\n",
       " 'NOISE',\n",
       " 'PSRR',\n",
       " 'PhaseMargin_PM',\n",
       " 'BandWidth']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Value out of range: 1231312693637327475383720003129487931408741852202045208373384168882678805359287831606695820465153613775207124697087",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-1f7296caba4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxxtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0myytrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0myytrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfile_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsvDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxxtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myytrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m final_train_loader = torch.utils.data.DataLoader(file_train,\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_fury/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_fury/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_fury/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m   8520\u001b[0m         \u001b[0mbegin_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbegin_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8521\u001b[0m         \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshrink_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshrink_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8522\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   8523\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8524\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_fury/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    701\u001b[0m                                      attr_def.allowed_values.list.s))))\n\u001b[1;32m    702\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"int\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m           \u001b[0mattr_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MakeInt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_minimum\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Value out of range: 1231312693637327475383720003129487931408741852202045208373384168882678805359287831606695820465153613775207124697087"
     ]
    }
   ],
   "source": [
    "xxtrain = X[a]\n",
    "yytrain = Y[a]\n",
    "yytrain.shape\n",
    "file_train = csvDataset(xxtrain,yytrain,transform= ToTensor())\n",
    "final_train_loader = torch.utils.data.DataLoader(file_train,\n",
    "                                             batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleConvNet()\n",
    "model = model.float().cuda()\n",
    "\n",
    "criterion = F.binary_cross_entropy_with_logits\n",
    "optimizer = VOGN(model, train_set_size=379, prec_init=1e2, num_samples=10)\n",
    "optimizer_bb = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0]:\n",
      "Iteration[101]:  Train Accuracy: 0.593668 \n",
      "## Epoch[0], Train Loss: 0.358958   &   Train Accuracy: 0.817942\n",
      "Epoch[1]:\n",
      "Iteration[101]:  Train Accuracy: 0.751979 \n",
      "## Epoch[1], Train Loss: 0.281827   &   Train Accuracy: 0.852243\n",
      "Epoch[2]:\n",
      "Iteration[101]:  Train Accuracy: 0.812665 \n",
      "## Epoch[2], Train Loss: 0.308817   &   Train Accuracy: 0.889182\n",
      "Epoch[3]:\n",
      "Iteration[101]:  Train Accuracy: 0.902375 \n",
      "## Epoch[3], Train Loss: 0.418837   &   Train Accuracy: 0.897098\n",
      "Epoch[4]:\n",
      "Iteration[101]:  Train Accuracy: 0.928760 \n",
      "## Epoch[4], Train Loss: 0.273771   &   Train Accuracy: 0.902375\n",
      "Epoch[5]:\n",
      "Iteration[101]:  Train Accuracy: 0.944591 \n",
      "## Epoch[5], Train Loss: 0.107490   &   Train Accuracy: 0.928760\n",
      "Epoch[6]:\n",
      "Iteration[101]:  Train Accuracy: 0.936675 \n",
      "## Epoch[6], Train Loss: 0.342532   &   Train Accuracy: 0.941953\n",
      "Epoch[7]:\n",
      "Iteration[101]:  Train Accuracy: 0.968338 \n",
      "## Epoch[7], Train Loss: 0.047915   &   Train Accuracy: 0.957784\n",
      "Epoch[8]:\n",
      "Iteration[101]:  Train Accuracy: 0.978892 \n",
      "## Epoch[8], Train Loss: 0.163977   &   Train Accuracy: 0.941953\n",
      "Epoch[9]:\n",
      "Iteration[101]:  Train Accuracy: 0.981530 \n",
      "## Epoch[9], Train Loss: 0.029814   &   Train Accuracy: 0.963061\n",
      "Epoch[10]:\n",
      "Iteration[101]:  Train Accuracy: 0.989446 \n",
      "## Epoch[10], Train Loss: 0.004863   &   Train Accuracy: 0.973615\n",
      "Epoch[11]:\n",
      "Iteration[101]:  Train Accuracy: 0.986807 \n",
      "## Epoch[11], Train Loss: 0.002208   &   Train Accuracy: 0.989446\n",
      "Epoch[12]:\n",
      "Iteration[101]:  Train Accuracy: 0.976253 \n",
      "## Epoch[12], Train Loss: 0.003174   &   Train Accuracy: 0.989446\n",
      "Epoch[13]:\n",
      "Iteration[101]:  Train Accuracy: 0.992084 \n",
      "## Epoch[13], Train Loss: 0.000627   &   Train Accuracy: 0.984169\n",
      "Epoch[14]:\n",
      "Iteration[101]:  Train Accuracy: 0.992084 \n",
      "## Epoch[14], Train Loss: 0.000800   &   Train Accuracy: 0.989446\n",
      "Epoch[15]:\n",
      "Iteration[101]:  Train Accuracy: 0.992084 \n",
      "## Epoch[15], Train Loss: 0.062666   &   Train Accuracy: 0.960422\n",
      "Epoch[16]:\n",
      "Iteration[101]:  Train Accuracy: 0.992084 \n",
      "## Epoch[16], Train Loss: 0.066063   &   Train Accuracy: 0.960422\n",
      "Epoch[17]:\n",
      "Iteration[101]:  Train Accuracy: 0.994723 \n",
      "## Epoch[17], Train Loss: 0.002735   &   Train Accuracy: 0.997361\n",
      "Epoch[18]:\n",
      "Iteration[101]:  Train Accuracy: 0.997361 \n",
      "## Epoch[18], Train Loss: 0.000395   &   Train Accuracy: 0.994723\n",
      "Epoch[19]:\n",
      "Iteration[101]:  Train Accuracy: 0.994723 \n",
      "## Epoch[19], Train Loss: 0.001069   &   Train Accuracy: 0.994723\n",
      "Epoch[20]:\n",
      "Iteration[101]:  Train Accuracy: 0.994723 \n",
      "## Epoch[20], Train Loss: 0.001199   &   Train Accuracy: 0.997361\n",
      "Epoch[21]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[21], Train Loss: 0.000141   &   Train Accuracy: 1.000000\n",
      "Epoch[22]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[22], Train Loss: 0.000052   &   Train Accuracy: 1.000000\n",
      "Epoch[23]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[23], Train Loss: 0.000027   &   Train Accuracy: 1.000000\n",
      "Epoch[24]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[24], Train Loss: 0.000017   &   Train Accuracy: 1.000000\n",
      "Epoch[25]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[25], Train Loss: 0.000011   &   Train Accuracy: 1.000000\n",
      "Epoch[26]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[26], Train Loss: 0.000008   &   Train Accuracy: 1.000000\n",
      "Epoch[27]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[27], Train Loss: 0.000005   &   Train Accuracy: 1.000000\n",
      "Epoch[28]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[28], Train Loss: 0.000004   &   Train Accuracy: 1.000000\n",
      "Epoch[29]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[29], Train Loss: 0.000003   &   Train Accuracy: 1.000000\n",
      "Epoch[30]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[30], Train Loss: 0.000002   &   Train Accuracy: 1.000000\n",
      "Epoch[31]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[31], Train Loss: 0.000002   &   Train Accuracy: 1.000000\n",
      "Epoch[32]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[32], Train Loss: 0.000001   &   Train Accuracy: 1.000000\n",
      "Epoch[33]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[33], Train Loss: 0.000001   &   Train Accuracy: 1.000000\n",
      "Epoch[34]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[34], Train Loss: 0.000001   &   Train Accuracy: 1.000000\n",
      "Epoch[35]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[35], Train Loss: 0.000000   &   Train Accuracy: 1.000000\n",
      "Epoch[36]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[36], Train Loss: 0.000000   &   Train Accuracy: 1.000000\n",
      "Epoch[37]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[37], Train Loss: 0.000000   &   Train Accuracy: 1.000000\n",
      "Epoch[38]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[38], Train Loss: 0.000000   &   Train Accuracy: 1.000000\n",
      "Epoch[39]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[39], Train Loss: 0.000000   &   Train Accuracy: 1.000000\n",
      "Epoch[40]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[40], Train Loss: 0.000000   &   Train Accuracy: 1.000000\n",
      "Epoch[41]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[41], Train Loss: 0.000000   &   Train Accuracy: 1.000000\n",
      "Epoch[42]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[42], Train Loss: 0.000000   &   Train Accuracy: 1.000000\n",
      "Epoch[43]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[43], Train Loss: 0.000000   &   Train Accuracy: 1.000000\n",
      "Epoch[44]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[44], Train Loss: 0.000000   &   Train Accuracy: 1.000000\n",
      "Epoch[45]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[45], Train Loss: 0.000000   &   Train Accuracy: 1.000000\n",
      "Epoch[46]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[46], Train Loss: 0.000000   &   Train Accuracy: 1.000000\n",
      "Epoch[47]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[47], Train Loss: 0.000000   &   Train Accuracy: 1.000000\n",
      "Epoch[48]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[48], Train Loss: 0.000000   &   Train Accuracy: 1.000000\n",
      "Epoch[49]:\n",
      "Iteration[101]:  Train Accuracy: 1.000000 \n",
      "## Epoch[49], Train Loss: 0.000000   &   Train Accuracy: 1.000000\n",
      "54.04266309738159\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model, train_loss, train_accuracy = train_model_cc(model, [final_train_loader, final_train_loader], criterion,\n",
    "optimizer_bb, num_epochs=50)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict = torch.zeros(100,6912).cuda()\n",
    "labz = torch.zeros(100,6912).cuda()\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    predictions,lbl = inference_bb(model, inference_loader,optimizer,1)\n",
    "    predict[i] = predictions.view(6912)\n",
    "    labz[i] = lbl.view(6912)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result = (np.sum(predict.cpu().numpy(),axis=0)>0.5)*1.\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels = (np.sum(labz.cpu().numpy(),axis=0)>0.5)*1.\n",
    "labels.shape,labels[4146:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(np.sum(result[4146:]==labels[4146:])/2766)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.615 ///0.656"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict_train = np.sum(predict[:,:4146].cpu().numpy(),axis=0)/100\n",
    "predict_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.hist(predict_train, bins='auto')\n",
    "plt.title(\"Histogram with 'auto' bins\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in inference_loader:\n",
    "        inputs = i['data']\n",
    "        labels = i['label']\n",
    "        if use_cuda:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        out = model.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (out.cpu().numpy()>0)*1.\n",
    "labels = (labels.cpu().numpy())*1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9150397686189443\n"
     ]
    }
   ],
   "source": [
    "correct = print(np.sum(pred[4146:]==labels[4146:])/2766)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active Learning for IC design by Ashish James, July 20, 2018\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import time\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from modAL.uncertainty import uncertainty_sampling\n",
    "from modAL.models import ActiveLearner, Committee\n",
    "\n",
    "##from models.model_clf import IC_Design_DNN_Clf\n",
    "\n",
    "scaler = StandardScaler()\n",
    "num_classes = 2\n",
    "REG_FLAG = False\n",
    "csv_file = \"circuit-design/opAmp_280nm_GF55_Mod_30P.csv\"  # Dataset1\n",
    "input_dims = 9\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class IC_Design_DNN_Clf:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, X, reuse=False):\n",
    "\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            dense1 = tf.layers.dense(inputs=X, units=64, activation=tf.nn.relu)\n",
    "            dense2 = tf.layers.dense(inputs=dense1, units=64, activation=tf.nn.relu)\n",
    "            # dense3 = tf.layers.dense(inputs=dense2, units=64, activation=tf.nn.relu)\n",
    "            outputs = tf.layers.dense(inputs=dense2, units=2, activation=tf.nn.relu)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    @property\n",
    "    def vars(self):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gregoire/anaconda3/envs/pytorch_fury/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/gregoire/anaconda3/envs/pytorch_fury/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_data(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    input = data.iloc[:,0:9]    \n",
    "    if REG_FLAG:\n",
    "        output = data.iloc[:,10:17]        \n",
    "    else:\n",
    "        output = data.iloc[:,9]\n",
    "        integer_encoded = LabelEncoder().fit_transform(output)\n",
    "        output = to_categorical(integer_encoded)\n",
    "    return input, output\n",
    "input, output = read_data(csv_file)\n",
    "\n",
    "#logdir = 'tf_logs/Inverse_Prob'\n",
    "#ckptdir = logdir + '/model'\n",
    "#if not os.path.exists(logdir):\n",
    "#    os.mkdir(logdir)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.random.set_random_seed(1)\n",
    "\n",
    "with tf.name_scope('Classifier'):\n",
    "    # Initialize neural network\n",
    "    DNN = IC_Design_DNN_Clf('DNN')\n",
    "    # Setup training process\n",
    "    lmda = tf.placeholder_with_default(0.01, shape=[], name='lambda')\n",
    "    X = tf.placeholder(tf.float32, [None, 9], name='X')\n",
    "    Y = tf.placeholder(tf.float32, [None, 2], name='Y')\n",
    "\n",
    "    tf.add_to_collection('placeholders', lmda)\n",
    "\n",
    "    Targets = DNN(X)\n",
    "    Targets_s = tf.nn.sigmoid(Targets)\n",
    "\n",
    "    # cost = tf.reduce_mean(tf.square(Targets-Y))\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Targets, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost, var_list=DNN.vars)\n",
    "\n",
    "    # correct_prediction = Targets - Y\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Targets_s, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    err_rate = 1 - accuracy\n",
    "\n",
    "cost_summary = tf.summary.scalar('Cost', cost)\n",
    "accuray_summary = tf.summary.scalar('Accuracy', accuracy)\n",
    "summary = tf.summary.merge_all()\n",
    "input_norm = scaler.fit_transform(input)\n",
    "scl_mean_ip = scaler.mean_\n",
    "scl_var_ip = scaler.var_\n",
    "# trainY = output #scaler.fit_transform(output)\n",
    "# scl_mean_op = scaler.mean_\n",
    "# scl_var_op = scaler.var_\n",
    "# Train, test and validation datasets\n",
    "trainX_tmp, testX, trainY_tmp, testY  = train_test_split(input_norm, output, test_size=0.2, random_state=1)\n",
    "trainX_tmp, valX, trainY_tmp, valY = train_test_split(trainX_tmp, trainY_tmp, test_size=0.25, random_state=1)\n",
    "\n",
    "sel_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_a=trainX_tmp[a]\n",
    "trainY_a=trainY_tmp[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_perf(trainX,trainY):\n",
    "    start = time.time()\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    training_epochs = 50\n",
    "    batch_size = 3\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(trainX.shape[0] / batch_size)\n",
    "\n",
    "        avg_cost = 0\n",
    "        avg_acc = 0\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            ran_from = i * batch_size\n",
    "            ran_to = (i + 1) * batch_size\n",
    "            batch_xs = trainX[ran_from:ran_to]\n",
    "            batch_ys = trainY[ran_from:ran_to]\n",
    "            # batch_ys = np.reshape(batch_ys, [batch_size, 2])\n",
    "            # batch_ys = batch_ys.values.reshape(batch_size, 1)\n",
    "            _, cc, aa, summary_str, tt, yy = sess.run([optimizer, cost, accuracy, summary, Targets, Y], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "\n",
    "            avg_cost += cc / total_batch\n",
    "            avg_acc += aa / total_batch\n",
    "\n",
    "            #file_writer.add_summary(summary_str, epoch * total_batch + i)\n",
    "\n",
    "        err_rate_val = sess.run(err_rate, feed_dict={X: valX, Y: valY})\n",
    "        if epoch%20==0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'Tsize =', '%d' % trainX.shape[0], 'cost =', '{:.9f}'.format(avg_cost), 'acc =', '{:.9f}'.format(avg_acc), 'err_r =', '{:.9f}'.format(err_rate_val),)\n",
    "\n",
    "    acc_test = sess.run(accuracy, feed_dict={X: testX, Y: testY})\n",
    "    print('Accuracy_Test =', '{:.9f}'.format(acc_test))\n",
    "        # saver.save(sess, ckptdir)\n",
    "\n",
    "    sess.close()\n",
    "\n",
    "    end = time.time()\n",
    "    print (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gregoire/anaconda3/envs/pytorch_fury/lib/python3.6/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Tsize = 379 cost = 0.694402404 acc = 0.510582022 err_r = 0.316702843\n",
      "Epoch: 0021 Tsize = 379 cost = 0.386672940 acc = 0.978835980 err_r = 0.089660168\n",
      "Epoch: 0041 Tsize = 379 cost = 0.356882867 acc = 1.000000000 err_r = 0.085321784\n",
      "Accuracy_Test = 0.904555321\n",
      "11.299848079681396\n"
     ]
    }
   ],
   "source": [
    "test_perf(trainX_a,trainY_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
