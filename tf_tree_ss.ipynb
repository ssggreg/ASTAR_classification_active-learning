{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Active Learning for IC design by Ashish James, July 20, 2018\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import time\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from modAL.uncertainty import uncertainty_sampling\n",
    "from modAL.models import ActiveLearner, Committee\n",
    "\n",
    "##from models.model_clf import IC_Design_DNN_Clf\n",
    "\n",
    "scaler = StandardScaler()\n",
    "num_classes = 2\n",
    "REG_FLAG = False\n",
    "csv_file = \"circuit-design/opAmp_280nm_GF55_Mod_30P.csv\"  # Dataset1\n",
    "input_dims = 9\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class IC_Design_DNN_Clf:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, X, reuse=False):\n",
    "\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            dense1 = tf.layers.dense(inputs=X, units=64, activation=tf.nn.relu)\n",
    "            dense2 = tf.layers.dense(inputs=dense1, units=64, activation=tf.nn.relu)\n",
    "            # dense3 = tf.layers.dense(inputs=dense2, units=64, activation=tf.nn.relu)\n",
    "            outputs = tf.layers.dense(inputs=dense2, units=2, activation=tf.nn.relu)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    @property\n",
    "    def vars(self):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gregoire/anaconda3/envs/pytorch_fury/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/gregoire/anaconda3/envs/pytorch_fury/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_data(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    input = data.iloc[:,0:9]    \n",
    "    if REG_FLAG:\n",
    "        output = data.iloc[:,10:17]        \n",
    "    else:\n",
    "        output = data.iloc[:,9]\n",
    "        integer_encoded = LabelEncoder().fit_transform(output)\n",
    "        output = to_categorical(integer_encoded)\n",
    "    return input, output\n",
    "input, output = read_data(csv_file)\n",
    "\n",
    "#logdir = 'tf_logs/Inverse_Prob'\n",
    "#ckptdir = logdir + '/model'\n",
    "#if not os.path.exists(logdir):\n",
    "#    os.mkdir(logdir)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.random.set_random_seed(1)\n",
    "\n",
    "with tf.name_scope('Classifier'):\n",
    "    # Initialize neural network\n",
    "    DNN = IC_Design_DNN_Clf('DNN')\n",
    "    # Setup training process\n",
    "    lmda = tf.placeholder_with_default(0.01, shape=[], name='lambda')\n",
    "    X = tf.placeholder(tf.float32, [None, 9], name='X')\n",
    "    Y = tf.placeholder(tf.float32, [None, 2], name='Y')\n",
    "\n",
    "    tf.add_to_collection('placeholders', lmda)\n",
    "\n",
    "    Targets = DNN(X)\n",
    "    Targets_s = tf.nn.sigmoid(Targets)\n",
    "\n",
    "    # cost = tf.reduce_mean(tf.square(Targets-Y))\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Targets, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost, var_list=DNN.vars)\n",
    "\n",
    "    # correct_prediction = Targets - Y\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Targets_s, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    err_rate = 1 - accuracy\n",
    "\n",
    "cost_summary = tf.summary.scalar('Cost', cost)\n",
    "accuray_summary = tf.summary.scalar('Accuracy', accuracy)\n",
    "summary = tf.summary.merge_all()\n",
    "input_norm = scaler.fit_transform(input)\n",
    "scl_mean_ip = scaler.mean_\n",
    "scl_var_ip = scaler.var_\n",
    "# trainY = output #scaler.fit_transform(output)\n",
    "# scl_mean_op = scaler.mean_\n",
    "# scl_var_op = scaler.var_\n",
    "# Train, test and validation datasets\n",
    "trainX_tmp, testX, trainY_tmp, testY  = train_test_split(input_norm, output, test_size=0.2, random_state=1)\n",
    "trainX_tmp, valX, trainY_tmp, valY = train_test_split(trainX_tmp, trainY_tmp, test_size=0.25, random_state=1)\n",
    "\n",
    "sel_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random\n",
      "0.031207561492919922\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "start = time.time()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#saver = tf.train.Saver()\n",
    "#file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "\n",
    "sel_met = 'random'\n",
    "#sel_met = 'active'\n",
    "\n",
    "print (sel_met)\n",
    "       # -----------------------------------\n",
    "if sel_met == 'random':\n",
    "    # --------------------------------------------------------------\n",
    "    # No.1 Randomly select the train samples\n",
    "    trainY_tmp_lab = np.argmax(trainY_tmp, axis=1)\n",
    "    fail_ind = np.argwhere(trainY_tmp_lab == 0)[:,0]\n",
    "    pass_ind = np.argwhere(trainY_tmp_lab == 1)[:,0]\n",
    "\n",
    "    trainX_fail_tmp = trainX_tmp[fail_ind]\n",
    "    trainY_fail_tmp = trainY_tmp[fail_ind]\n",
    "    trainX_pass_tmp = trainX_tmp[pass_ind]\n",
    "    trainY_pass_tmp = trainY_tmp[pass_ind]\n",
    "\n",
    "    fail_ran = np.random.permutation(fail_ind.shape[0])\n",
    "    fail_sel = fail_ran[np.arange(0, np.int(np.dot(fail_ind.shape[0], sel_rate)), 1)]\n",
    "    pass_ran = np.random.permutation(pass_ind.shape[0])\n",
    "    pass_sel = pass_ran[np.arange(0, np.int(np.dot(pass_ind.shape[0], sel_rate)), 1)]\n",
    "\n",
    "    trainX_pass = trainX_pass_tmp[pass_sel]\n",
    "    trainY_pass = trainY_pass_tmp[pass_sel]\n",
    "    trainX_fail = trainX_fail_tmp[fail_sel]\n",
    "    trainY_fail = trainY_fail_tmp[fail_sel]\n",
    "\n",
    "    trainX = np.vstack((trainX_pass, trainX_fail))\n",
    "    trainY = np.vstack((trainY_pass, trainY_fail))\n",
    "\n",
    "else :\n",
    "    #---------------------------------------------------\n",
    "    #active learning  \n",
    "    #import pdb; pdb.set_trace()\n",
    "    n_members = 2\n",
    "    learner_list = list()\n",
    "\n",
    "    for member_idx in range(n_members):\n",
    "        # initial training data\n",
    "        n_initial = 2\n",
    "        initial_idx = np.random.choice(range(trainX_tmp.shape[0]), size=n_initial, replace=False)\n",
    "        trainX_initial = trainX_tmp[initial_idx]\n",
    "        trainY_initial = trainY_tmp[initial_idx][:,0]\n",
    "\n",
    "        trainX_pool = np.delete(trainX_tmp, initial_idx, axis = 0)\n",
    "        trainY_pool = np.delete(trainY_tmp[:,0], initial_idx)\n",
    "        trainY_pool_org = np.delete(trainY_tmp, initial_idx, axis = 0)\n",
    "\n",
    "        # initializing the active learner\n",
    "        learner = ActiveLearner(\n",
    "            estimator = RandomForestClassifier(),\n",
    "            # query_strategy = uncertainty_sampling,   \n",
    "            X_training = trainX_initial, y_training = trainY_initial\n",
    "            )\n",
    "        \n",
    "        learner_list.append(learner)\n",
    "\n",
    "    committee = Committee(learner_list = learner_list)\n",
    "\n",
    "    # unqueried_score = committee.score(trainX_tmp,trainY_tmp[:,0])\n",
    "    # performance_history = [unqueried_score]\n",
    "    # active learning\n",
    "    n_queries = np.int(np.dot(trainX_tmp.shape[0], sel_rate)) - n_initial\n",
    "\n",
    "    trainX = np.zeros(shape = (n_queries, 9))\n",
    "    trainY = np.zeros(shape = (n_queries, 2))\n",
    "    for idx in range(n_queries):\n",
    "        query_idx, query_instance = committee.query(trainX_pool)  \n",
    "        #print(query_idx)\n",
    "\n",
    "        committee.teach(trainX_pool[query_idx].reshape(1,-1), trainY_pool[query_idx]) \n",
    "        # performance_history.append(committee.score(trainX_tmp, trainY_tmp[:,0]))\n",
    "        trainX[idx] = trainX_pool[query_idx]\n",
    "        trainY[idx] = trainY_pool_org[query_idx]\n",
    "        trainX_pool, trainY_pool, trainY_pool_org = np.delete(trainX_pool, query_idx, axis = 0), np.delete(trainY_pool, query_idx, axis = 0), np.delete(trainY_pool_org, query_idx, axis = 0)\n",
    "# --------------------------------------------------------------------------------\n",
    "end = time.time()\n",
    "print (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Tsize = 414 cost = 0.723011245 acc = 0.688405800 err_r = 0.311641335\n",
      "Epoch: 0002 Tsize = 414 cost = 0.692713562 acc = 0.700483093 err_r = 0.295733929\n",
      "Epoch: 0003 Tsize = 414 cost = 0.693253683 acc = 0.717391309 err_r = 0.275488079\n",
      "Epoch: 0004 Tsize = 414 cost = 0.685360702 acc = 0.758454111 err_r = 0.253073037\n",
      "Epoch: 0005 Tsize = 414 cost = 0.677946677 acc = 0.785024160 err_r = 0.229934931\n",
      "Epoch: 0006 Tsize = 414 cost = 0.623142719 acc = 0.826086963 err_r = 0.309472144\n",
      "Epoch: 0007 Tsize = 414 cost = 0.588842342 acc = 0.797101455 err_r = 0.248734653\n",
      "Epoch: 0008 Tsize = 414 cost = 0.535969623 acc = 0.845410633 err_r = 0.204627633\n",
      "Epoch: 0009 Tsize = 414 cost = 0.507065534 acc = 0.864734305 err_r = 0.194504678\n",
      "Epoch: 0010 Tsize = 414 cost = 0.490767821 acc = 0.874396140 err_r = 0.180766463\n",
      "Epoch: 0011 Tsize = 414 cost = 0.473746801 acc = 0.888888894 err_r = 0.174258828\n",
      "Epoch: 0012 Tsize = 414 cost = 0.458169355 acc = 0.910628023 err_r = 0.167751253\n",
      "Epoch: 0013 Tsize = 414 cost = 0.447290737 acc = 0.917874400 err_r = 0.165582061\n",
      "Epoch: 0014 Tsize = 414 cost = 0.436216054 acc = 0.922705317 err_r = 0.156905293\n",
      "Epoch: 0015 Tsize = 414 cost = 0.429935896 acc = 0.932367153 err_r = 0.147505403\n",
      "Epoch: 0016 Tsize = 414 cost = 0.417692008 acc = 0.937198070 err_r = 0.140274763\n",
      "Epoch: 0017 Tsize = 414 cost = 0.407216310 acc = 0.956521741 err_r = 0.139551699\n",
      "Epoch: 0018 Tsize = 414 cost = 0.401385885 acc = 0.956521742 err_r = 0.133044124\n",
      "Epoch: 0019 Tsize = 414 cost = 0.398375931 acc = 0.963768118 err_r = 0.131597996\n",
      "Epoch: 0020 Tsize = 414 cost = 0.395820942 acc = 0.963768118 err_r = 0.132321060\n",
      "Epoch: 0021 Tsize = 414 cost = 0.392095229 acc = 0.963768118 err_r = 0.127259552\n",
      "Epoch: 0022 Tsize = 414 cost = 0.388552241 acc = 0.968599036 err_r = 0.120751977\n",
      "Epoch: 0023 Tsize = 414 cost = 0.382694761 acc = 0.971014494 err_r = 0.121475041\n",
      "Epoch: 0024 Tsize = 414 cost = 0.379595444 acc = 0.968599036 err_r = 0.124367297\n",
      "Epoch: 0025 Tsize = 414 cost = 0.379051964 acc = 0.971014494 err_r = 0.118582785\n",
      "Epoch: 0026 Tsize = 414 cost = 0.380534566 acc = 0.973429953 err_r = 0.122921169\n",
      "Epoch: 0027 Tsize = 414 cost = 0.372659691 acc = 0.975845412 err_r = 0.120751977\n",
      "Epoch: 0028 Tsize = 414 cost = 0.368906088 acc = 0.978260871 err_r = 0.112798274\n",
      "Epoch: 0029 Tsize = 414 cost = 0.368186773 acc = 0.975845412 err_r = 0.118582785\n",
      "Epoch: 0030 Tsize = 414 cost = 0.367307351 acc = 0.985507247 err_r = 0.119305849\n",
      "Epoch: 0031 Tsize = 414 cost = 0.368855596 acc = 0.980676330 err_r = 0.117859721\n",
      "Epoch: 0032 Tsize = 414 cost = 0.365644382 acc = 0.983091788 err_r = 0.114244401\n",
      "Epoch: 0033 Tsize = 414 cost = 0.362831354 acc = 0.983091788 err_r = 0.112075210\n",
      "Epoch: 0034 Tsize = 414 cost = 0.361763720 acc = 0.990338165 err_r = 0.117136657\n",
      "Epoch: 0035 Tsize = 414 cost = 0.361633717 acc = 0.990338165 err_r = 0.117136657\n",
      "Epoch: 0036 Tsize = 414 cost = 0.357814839 acc = 0.992753624 err_r = 0.117136657\n",
      "Epoch: 0037 Tsize = 414 cost = 0.358328041 acc = 0.992753624 err_r = 0.113521338\n",
      "Epoch: 0038 Tsize = 414 cost = 0.357960211 acc = 0.997584541 err_r = 0.107013762\n",
      "Epoch: 0039 Tsize = 414 cost = 0.357757383 acc = 0.995169082 err_r = 0.118582785\n",
      "Epoch: 0040 Tsize = 414 cost = 0.355853354 acc = 0.997584541 err_r = 0.112798274\n",
      "Epoch: 0041 Tsize = 414 cost = 0.353250968 acc = 0.997584541 err_r = 0.112798274\n",
      "Epoch: 0042 Tsize = 414 cost = 0.353121302 acc = 0.997584541 err_r = 0.111352146\n",
      "Epoch: 0043 Tsize = 414 cost = 0.353699348 acc = 0.997584541 err_r = 0.114967465\n",
      "Epoch: 0044 Tsize = 414 cost = 0.355990473 acc = 0.992753624 err_r = 0.119305849\n",
      "Epoch: 0045 Tsize = 414 cost = 0.353873067 acc = 0.997584541 err_r = 0.111352146\n",
      "Epoch: 0046 Tsize = 414 cost = 0.355359049 acc = 0.995169082 err_r = 0.109182954\n",
      "Epoch: 0047 Tsize = 414 cost = 0.351481885 acc = 0.997584541 err_r = 0.107736826\n",
      "Epoch: 0048 Tsize = 414 cost = 0.350751704 acc = 0.997584541 err_r = 0.115690529\n",
      "Epoch: 0049 Tsize = 414 cost = 0.349747550 acc = 0.997584541 err_r = 0.117859721\n",
      "Epoch: 0050 Tsize = 414 cost = 0.352784516 acc = 0.997584541 err_r = 0.116413593\n",
      "Accuracy_Test = 0.879971087\n",
      "25.76756525039673\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "training_epochs = 50\n",
    "batch_size = 3\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(trainX.shape[0] / batch_size)\n",
    "    \n",
    "    avg_cost = 0\n",
    "    avg_acc = 0\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        ran_from = i * batch_size\n",
    "        ran_to = (i + 1) * batch_size\n",
    "        batch_xs = trainX[ran_from:ran_to]\n",
    "        batch_ys = trainY[ran_from:ran_to]\n",
    "        batch_ys = np.reshape(batch_ys, [batch_size, 2])\n",
    "        # batch_ys = batch_ys.values.reshape(batch_size, 1)\n",
    "        _, cc, aa, summary_str, tt, yy = sess.run([optimizer, cost, accuracy, summary, Targets, Y], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "        \n",
    "        avg_cost += cc / total_batch\n",
    "        avg_acc += aa / total_batch\n",
    "\n",
    "        #file_writer.add_summary(summary_str, epoch * total_batch + i)\n",
    "\n",
    "    err_rate_val = sess.run(err_rate, feed_dict={X: valX, Y: valY})\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'Tsize =', '%d' % trainX.shape[0], 'cost =', '{:.9f}'.format(avg_cost), 'acc =', '{:.9f}'.format(avg_acc), 'err_r =', '{:.9f}'.format(err_rate_val),)\n",
    "\n",
    "acc_test = sess.run(accuracy, feed_dict={X: testX, Y: testY})\n",
    "print('Accuracy_Test =', '{:.9f}'.format(acc_test))\n",
    "    # saver.save(sess, ckptdir)\n",
    "\n",
    "sess.close()\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
